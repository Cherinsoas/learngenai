{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Memory"
      ],
      "metadata": {
        "id": "TdKG_EDTawzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O625tmIiarjO",
        "outputId": "0c36e87a-6acc-4722-9e3c-b6a8322f62fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.7/418.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-community langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "id": "bDnt7sBubYiu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "BPVuK9t7bfGo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY']=OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "4qjuIpaAbiQS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "rAB0vk92bAIW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")"
      ],
      "metadata": {
        "id": "xZTkrkzNbQNE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage"
      ],
      "metadata": {
        "id": "wyQ3K9e1bU3a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "id": "zr5qqAJ9cMQC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"you are a helpful assistant. Answer all the question to the best of your ability.\"\n",
        "    ),\n",
        "    MessagesPlaceholder(variable_name = \"messages\"),\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "NT6i6GFhcUFP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain =  prompt | llm"
      ],
      "metadata": {
        "id": "H4zN6-A5c2xa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposne = chain.invoke(\n",
        "    {\n",
        "       \"messages\": [\n",
        "\n",
        "                   HumanMessage(\n",
        "                       content= \"translate this given sentence from english to french: I LOVE AI.\"\n",
        "                   ),\n",
        "                   AIMessage(content= \"J'adore la AI.\"),\n",
        "\n",
        "                   HumanMessage(content= \"what did you just say?\"),\n",
        "       ] ,\n",
        "\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "pJ4L2FuAc_Fe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposne.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gvz8k8dmd_6y",
        "outputId": "86123d65-0467-4d2d-d545-f648aff9c575"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I said \"I love AI\" in French.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory"
      ],
      "metadata": {
        "id": "L5fyhefUeMkp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history=ChatMessageHistory()"
      ],
      "metadata": {
        "id": "QD2wlPj0e94o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_user_message(\"translate this given sentence from english to french: I LOVE AI.\")"
      ],
      "metadata": {
        "id": "yJNtOUetgAMh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_ai_message(\"J'adore la AI.\")"
      ],
      "metadata": {
        "id": "5AnLRjm7gEiq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxLeaC71gRxR",
        "outputId": "5a9cc3d4-0f50-417f-eac6-a7d8a79c9631"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='translate this given sentence from english to french: I LOVE AI.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"J'adore la AI.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history=ChatMessageHistory()"
      ],
      "metadata": {
        "id": "OC4-sihvgVHH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input1=\"who is a prime minister of india?\""
      ],
      "metadata": {
        "id": "CLTVvsxIgi3g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_user_message(input1)"
      ],
      "metadata": {
        "id": "5H5_BFi1gvi4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj1jatjzg7k5",
        "outputId": "20004f83-26a9-4917-de41-41f8e11a5d13"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='who is a prime minister of india?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposne=chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_chat_history.messages\n",
        "    }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "mv9vdH_xg0Ml"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposne.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "hYp7_AkahASE",
        "outputId": "20607e8a-045b-4047-c102-113bc49d4115"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As of October 2021, the Prime Minister of India is Narendra Modi. He has been in office since May 2014 after his party, the Bharatiya Janata Party (BJP), won the general elections.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_ai_message(resposne)"
      ],
      "metadata": {
        "id": "_c-OiJekhLGh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input2=\"what did i ask to you just now?\""
      ],
      "metadata": {
        "id": "QjIaG2PAhXtf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_user_message(input2)"
      ],
      "metadata": {
        "id": "JrGpmtuUhb96"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qhJgQEzhkxC",
        "outputId": "afb0216e-e9d6-42ad-d98a-28f5ba8cc9e2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='who is a prime minister of india?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='As of October 2021, the Prime Minister of India is Narendra Modi. He has been in office since May 2014 after his party, the Bharatiya Janata Party (BJP), won the general elections.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 36, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_c66b5540ac', 'id': 'chatcmpl-BExJPadtYggQZMuUqlyzU2eq5CksV', 'finish_reason': 'stop', 'logprobs': None}, id='run-819996d0-24c2-4617-b8f6-0f8b15777350-0', usage_metadata={'input_tokens': 36, 'output_tokens': 47, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='what did i ask to you just now?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_chat_history.messages\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "up6yUSfwhm1P"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OK-G9O5lh2AB",
        "outputId": "45817ab9-48e5-4b58-83c2-081936599005"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You asked about the Prime Minister of India, and I provided the current Prime Minister's name.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain2 = prompt | llm"
      ],
      "metadata": {
        "id": "tMq_VhHCo0sg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt3 | llm\n"
      ],
      "metadata": {
        "id": "IU38-INhwopF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "I17yDS0Sm2TY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history2 = ChatMessageHistory()"
      ],
      "metadata": {
        "id": "UlQIjyainWFe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "                              chain,\n",
        "                              lambda session_id: demo_chat_history2,\n",
        "                              input_message_key = \"input\",\n",
        "                              history_message_key =\"chat_history\",\n",
        "                              )"
      ],
      "metadata": {
        "id": "gviZ8zBVnDJj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Translate this sentence from English to French: I love programming.\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDXkULIXw043",
        "outputId": "cb9d6c30-9967-4015-c1f7-59924a04d113"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 52, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_c66b5540ac', 'id': 'chatcmpl-BExK5A5m0NOEVep1syPZWgxF7RXuB', 'finish_reason': 'stop', 'logprobs': None}, id='run-8abed242-7a52-4abf-a2a2-4f5970ea43d9-0', usage_metadata={'input_tokens': 52, 'output_tokens': 21, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8ZfiLzdpEI2",
        "outputId": "b06a5ac4-9daa-408e-8702-00e2fd0b66ff"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You asked me to translate the sentence \"I love programming\" from English to French, and I provided the translation as \"J\\'adore programmer.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 351, 'total_tokens': 382, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_c66b5540ac', 'id': 'chatcmpl-BExKBkvcaEIQHmOstl1APtXhCAN2a', 'finish_reason': 'stop', 'logprobs': None}, id='run-63d007ec-e2f7-4f6b-b68d-faa52528f6be-0', usage_metadata={'input_tokens': 351, 'output_tokens': 31, 'total_tokens': 382, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI"
      ],
      "metadata": {
        "id": "Tzc8ItzeqqzX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''llm = OpenAI(\n",
        "\ttemperature=0,\n",
        " model_name=\"gpt-3.5-turbo-1106\"\n",
        ")\n",
        " '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4j8Z8cT1rt7g",
        "outputId": "7e3babd0-fbcb-48ad-c349-8f23d728586f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'llm = OpenAI(\\n\\ttemperature=0,\\n model_name=\"gpt-3.5-turbo-1106\"\\n)\\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain"
      ],
      "metadata": {
        "id": "z_4FjdcQr5ju"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm)"
      ],
      "metadata": {
        "id": "kUYI6V6QsAOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1f9b1d-b40b-46d8-f6ab-839b68494c19"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-07a8be19d9f8>:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(llm=llm)\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:214: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ROVWwlLsHlH",
        "outputId": "8fa60b30-e132-44c3-9854-936f8c326e23"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1=\"i am interested in exploring the potential of large language model with external knowledge\"\n",
        "question2=\"i would like to analysis all the different possibility, what can you think of?\"\n",
        "question3=\"which data source tyoe could be used to give context to the model?\""
      ],
      "metadata": {
        "id": "th4Pp8bysQvn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "rPnzVSQFtiGX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory= ConversationBufferMemory()\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "7-RFErhYtiYN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(\"good morning my AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqhGlC_etwTy",
        "outputId": "1a46f677-6a9e-4ef7-a177-6e773d40672a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-7bcbf6b1846f>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  conversation_buf(\"good morning my AI\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'good morning my AI',\n",
              " 'history': '',\n",
              " 'response': 'Good morning! How are you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback"
      ],
      "metadata": {
        "id": "8-yBfPVrt1Co"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(chain,query):\n",
        "  with get_openai_callback() as cb:\n",
        "    result= chain.run(query)\n",
        "    print(f\"totla no of token is {cb.total_tokens}\")\n",
        "  return result"
      ],
      "metadata": {
        "id": "lQ2nwZdvup3r"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1=\"i am interested in exploring the potential of large language model with external knowledge\""
      ],
      "metadata": {
        "id": "0FQCKtTAvMkg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(conversation_buf,question1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "54oRwJ3du-8k",
        "outputId": "d814ec21-4b97-455f-cca5-3b41302b23e5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-f921a4badc7d>:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result= chain.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totla no of token is 197\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's a fascinating topic! Large language models like GPT-3 are trained on vast amounts of text data, which allows them to generate human-like responses and even make inferences based on external knowledge sources. There are also efforts to integrate external knowledge bases like Wikipedia or scientific literature into these models to enhance their understanding and reasoning capabilities. It's an exciting area of research with a lot of potential for applications in natural language understanding and generation. Is there something specific you'd like to know about this topic?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-cax7oQvS-A",
        "outputId": "159a80ff-73b8-46e1-e395-0a450def9952"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'i am interested in exploring the potential of large language model with external knowledge',\n",
              " 'history': \"Human: good morning my AI\\nAI: Good morning! How are you today?\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: That's a fascinating topic! Large language models like GPT-3 are trained on vast amounts of text data, which allows them to generate human-like responses and even make inferences based on external knowledge sources. There are also efforts to integrate external knowledge bases like Wikipedia or scientific literature into these models to enhance their understanding and reasoning capabilities. It's an exciting area of research with a lot of potential for applications in natural language understanding and generation. Is there something specific you'd like to know about this topic?\",\n",
              " 'response': \"As I mentioned earlier, integrating external knowledge sources into large language models can enhance their understanding and reasoning capabilities. This can be especially useful in tasks like question answering, language translation, summarization, and dialogue generation. By leveraging external knowledge, these models can provide more accurate and contextually relevant responses, leading to improved performance in various natural language processing applications. There are ongoing research and development efforts in this area, and it's an exciting time to explore the potential of these models with external knowledge. Let me know if there's anything specific you'd like to delve into further!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question2=\"i would like to analysis all the different possibility, what can you think of?\""
      ],
      "metadata": {
        "id": "Xusxael4vcZ-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYf3GnPxvlKI",
        "outputId": "ccf6127e-95a1-4771-9e1c-0f7e85a30026"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'i would like to analysis all the different possibility, what can you think of?',\n",
              " 'history': \"Human: good morning my AI\\nAI: Good morning! How are you today?\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: That's a fascinating topic! Large language models like GPT-3 are trained on vast amounts of text data, which allows them to generate human-like responses and even make inferences based on external knowledge sources. There are also efforts to integrate external knowledge bases like Wikipedia or scientific literature into these models to enhance their understanding and reasoning capabilities. It's an exciting area of research with a lot of potential for applications in natural language understanding and generation. Is there something specific you'd like to know about this topic?\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: As I mentioned earlier, integrating external knowledge sources into large language models can enhance their understanding and reasoning capabilities. This can be especially useful in tasks like question answering, language translation, summarization, and dialogue generation. By leveraging external knowledge, these models can provide more accurate and contextually relevant responses, leading to improved performance in various natural language processing applications. There are ongoing research and development efforts in this area, and it's an exciting time to explore the potential of these models with external knowledge. Let me know if there's anything specific you'd like to delve into further!\",\n",
              " 'response': 'Some potential applications of large language models with external knowledge include:\\n\\n1. Question answering: These models can provide more accurate and informative answers by leveraging knowledge sources like Wikipedia or other databases.\\n\\n2. Language translation: By incorporating external knowledge, these models can improve the accuracy and contextuality of translated text.\\n\\n3. Summarization: Large language models can use external knowledge to generate more informative and concise summaries of text documents.\\n\\n4. Dialogue generation: By integrating external knowledge, these models can have more meaningful and contextually relevant conversations.\\n\\n5. Information retrieval: These models can benefit from external knowledge sources to better understand and retrieve relevant information.\\n\\n6. Personalized content generation: Utilizing external knowledge can help these models create personalized and context-aware content for users.\\n\\nThese are just a few examples, and there are many other possibilities to explore in this exciting area of research!'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(conversation_buf,question2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "P1YQb48UvnVY",
        "outputId": "50e4b69b-764a-4c0c-dca8-4231116c0ef7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totla no of token is 714\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some potential applications of large language models with external knowledge include:\\n\\n1. Question answering: These models can provide more accurate and informative answers by leveraging knowledge sources like Wikipedia or other databases.\\n\\n2. Language translation: By incorporating external knowledge, these models can improve the accuracy and contextuality of translated text.\\n\\n3. Summarization: Large language models can use external knowledge to generate more informative and concise summaries of text documents.\\n\\n4. Dialogue generation: By integrating external knowledge, these models can have more meaningful and contextually relevant conversations.\\n\\n5. Information retrieval: These models can benefit from external knowledge sources to better understand and retrieve relevant information.\\n\\n6. Personalized content generation: Utilizing external knowledge can help these models create personalized and context-aware content for users.\\n\\nThese are just a few examples, and there are many other possibilities to explore in this exciting area of research!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some potential areas of exploration could include the impact of different types of external knowledge sources on the performance of large language models, the development of more efficient algorithms for integrating external knowledge, and the ethical considerations of using external knowledge in language generation. Additionally, you could also explore the potential applications of large language models with external knowledge in various industries such as healthcare, finance, and customer service. There are also opportunities to investigate the potential limitations and challenges of these models, such as biases in the external knowledge sources and the potential for misinformation. These are just a few possibilities, and there are many more aspects to consider in exploring the potential of large language models with external knowledge."
      ],
      "metadata": {
        "id": "kG0jJPrMv4hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hIqJqOIAvyGB",
        "outputId": "61a5b5df-1c33-4e5b-f49e-d42f29108360"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'which data source tyoe could be used to give context to the model?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question3)['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "UOINeOzOwEJ8",
        "outputId": "ad1fc799-3660-4561-935d-8de5876585a8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'External knowledge sources such as open data repositories, scientific literature, encyclopedias, databases, and domain-specific knowledge bases can be used to provide context to large language models. These sources can contain valuable information and facts that can help the model understand and generate more accurate and contextually relevant responses. Additionally, leveraging structured data sources, such as knowledge graphs or ontologies, can also provide a rich source of context for the model to enhance its understanding and reasoning capabilities. By integrating these diverse data sources, large language models can improve their ability to generate human-like responses and make inferences based on external knowledge.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# take atleast 5 data source(1 ppt, 1 weblodaer, 1 pdf, 1txt, 1csv)\n",
        "\n",
        "# maintain one knowledge base(mongodb, astradb , pineconde, weviate)\n",
        "\n",
        "# user will ask you have to provide answer based on the asked question using this know;ledge base\n",
        "\n",
        "# handle the comman question as well like hi hello how are you good evening good morning etc.\n",
        "\n",
        "# you have to mention the complete memory of the conversation here the threshold is 10\n",
        "\n",
        "# then create a UI for you bot\n",
        "\n",
        "https://www.youtube.com/@sunnysavita10/videos\n",
        "\n"
      ],
      "metadata": {
        "id": "lwLzzCqZxec0"
      }
    }
  ]
}